#!/usr/bin/env python3
import argparse
from json import loads
from itertools import islice
from subprocess import check_call, check_output
from typing import List, Tuple, Dict, Type
import logging
import sys
from pathlib import Path
from os.path import basename
from collections import Counter

from common import get_logger, setup_paths
setup_paths()
from jsonify import from_json

from kython.logging import setup_logzero

OUTPUTS = Path(__file__).parent.joinpath('outputs').resolve()

class RepoHandle:
    def __init__(self, repo: str):
        self.repo = repo

    def check_output(self, *args):
        return check_output([
            'git', f'--git-dir={self.repo}/.git', *args
        ])

    def get_revisions(self) -> List[Tuple[str, str]]:
        ss = list(reversed(self.check_output(
            'log',
            '--pretty=format:%h %ad',
            '--no-patch',
        ).decode('utf8').splitlines()))
        return [(l.split()[0], ' '.join(l.split()[1:])) for l in ss]

    def get_content(self, rev: str) -> str:
        return self.check_output(
            'show',
            rev + ':content.json',
        ).decode('utf8')

    def get_all_versions(self):
        revs = self.get_revisions()
        jsons = []
        for rev, dd in revs:
            cc = self.get_content(rev)
            if len(cc.strip()) == 0:
                j = {}
            else:
                j = loads(cc)
            jsons.append((rev, dd, j))
        return jsons

def diffference(before, after):
    db = {x.uid: x for x in before}
    da = {x.uid: x for x in after}
    removed = []
    added = []
    for x in {*db.keys(), *da.keys()}:
        if x in db and x not in da:
            removed.append(db[x])
        elif x not in db and x in da:
            added.append(da[x])
        elif x in db and x in da:
            pass # TODO compare??
        else:
            raise AssertionError
    return removed, added

class Collector:
    def __init__(self):
        self.items = {}

    def register(self, batch):
        added = []
        for i in batch:
            if i.uid in self.items:
                pass # TODO FIXME compare? if description or tags changed, report it?
            else:
                added.append(i)
                self.items[i.uid] = i
        return added

def tabulate(text: str):
    if text is None:
        return "   "
    return '\n'.join('   ' + t for t in text.splitlines())

# TODO need some sort of starting_from??
# TODO I guess just use datetime?

def format_thing(r, ustats=None):
    us = '' if ustats is None else f'({ustats[r.user]:.3f})'
    tagss = ' '.join([f"<a href='https://pinboard.in/u:{r.user}/t:{t}'>{t}</a>" for t in r.tags])
    BB = f"""----------
{r.title}  {r.link}
{tabulate(r.description)}
tags: {tagss}
{r.when.strftime("%Y-%m-%d %H:%M")} by {r.user} {us} {r.blink}
----------
"""
    return BB

# TODO maybe, move to jsonify?..
def get_result_type(repo: str) -> Type:
    name = basename(repo)
    if name.startswith('reddit'):
        from reach import Result # type: ignore
        return Result
    elif name.startswith('github'):
        from tentacle import Result # type: ignore
        return Result
    else:
        from spinboard import Result # type: ignore
        return Result


# TODO hmm. instead percentile would be more accurate?...
def get_user_stats(jsons, rtype=None):
    cc = Collector()
    for jj in jsons:
        rev, dd, j = jj
        items = list(map(lambda x: from_json(rtype, x), j))
        cc.register(items)
    cnt = Counter([i.user for i in cc.items.values()])
    total = max(sum(cnt.values()), 1)
    return {
        u: v / total for u, v in cnt.items()
    }

# TODO html mode??
def get_digest(repo: str, count=None):
    rtype = get_result_type(repo)

    if count is None:
        count = 100

    rh = RepoHandle(repo)
    jsons = rh.get_all_versions()
    ustats = get_user_stats(jsons, rtype=rtype)

    # TODO shit. should have stored metadata in repository?... for now guess from filename..

    all_added = []
    cc = Collector()
    for jj in jsons[1:]:
        rev, dd, j = jj
        items = list(map(lambda x: from_json(rtype, x), j))
        added = cc.register(items)
        #print(f'revision {rev}: total {len(cc.items)}')
        #print(f'added {len(added)}')
        # if first:
        if len(added) == 0:
            continue
        all_added.append(f"++++{dd} rev {rev}+++++")
        all_added.extend([format_thing(x, ustats=ustats) for x in sorted(added, key=lambda e: e.when)]) # , reverse=True))
        # TODO link to user
        # TODO user weight?? count is fine I suppose...
        all_added.append(f"++++{dd} rev {rev}+++++")
        # TODO added date
#        if len(added) > 0:
#            for r in sorted(added, key=lambda r: r.uid):
#                # TODO link to bookmark
#                # TODO actually chould even generate html here...
#                # TODO highlight interesting users
#                # TODO how to track which ones were already notified??
#                # TODO I guess keep latest revision in a state??
#                print(BB)

    latest = islice(reversed(all_added), 0, count)
    return '\n'.join(latest)

# TODO search is a bit of flaky: initially I was getting
# so like exact opposites
# I guess removed links are basically not interesting, so we want to track whatever new was added

import requests
def send(subject: str, body: str, html=False):
    maybe_html: Dict[str, str] = {}
    if html:
        body = body.replace('\n', '\n<br>')
        maybe_html = {'html': body}
    return requests.post(
        "https://api.mailgun.net/v3/***REMOVED***.mailgun.org/messages",
        auth=(
            "api",
            "***REMOVED***" # TODO secrets..
        ),
        data={"from": "spinboard <mailgun@***REMOVED***.mailgun.org>",
              "to": ["karlicoss@gmail.com"],
              "subject": f"Spinboard stats for {subject}",
              # "text": body,
              **maybe_html,
        }
    )

def handle_one(repo: str, html=False, email=True):
    digest = get_digest(repo)
    if len(digest) == 0:
        digest = "INITIAL!!!!"
    if email:
        res = send(
            subject=basename(repo),
            body=digest,
            html=html,
        )
        res.raise_for_status()
    else:
        print("Skipping email!")


# TODO for starters, just send last few days digest..
def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('repo', nargs='?')
    parser.add_argument('--no-email', action='store_false', dest='email')
    args = parser.parse_args()

    logger = get_logger()
    setup_logzero(logger, level=logging.DEBUG)

    # parser.add_argument('--from', default=None)
    # parser.add_argument('--to', default=None)
    # froms = getattr(args, 'from')
    # TODO utc timestamp??
    # tos = args.to
    # TODO strptime?
    repos = []
    if args.repo is not None:
        repos = [OUTPUTS.joinpath(args.repo)]
    else:
        repos = [x for x in OUTPUTS.iterdir() if x.is_dir()]
    ok = True
    for repo in repos:
        try:
            logger.info("Processing %s", repo)
            handle_one(str(repo), html=True, email=args.email)
        except Exception as e:
            logger.exception(e)
            ok = False

    if not ok:
        sys.exit(1)




if __name__ == '__main__':
    main()
# TODO how to make it generic to incorporate github??


# basically a thing that knows how to fetch items with timestsamps
# and notify of new ones..
